# -*- coding:utf-8 -*-
##
## This file is part of Invenio.
## Copyright (C) 2010, 2011, 2012, 2014 CERN.
##
## Invenio is free software; you can redistribute it and/or
## modify it under the terms of the GNU General Public License as
## published by the Free Software Foundation; either version 2 of the
## License, or (at your option) any later version.
##
## Invenio is distributed in the hope that it will be useful, but
## WITHOUT ANY WARRANTY; without even the implied warranty of
## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
## General Public License for more details.
##
## You should have received a copy of the GNU General Public License
## along with Invenio; if not, write to the Free Software Foundation, Inc.,
## 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.
"""
    BibIndexTokenizer: generic, not implemented tokenizer for inheritance

    Inheritance tree for tokenizers in Invenio:

    BibIndexTokenizer
    ^
    |
    |----BibIndexStringTokenizer<---|
    |                               |
    |                       BibIndexDefaultTokenizer<---|
    |                                                   |
    |                                       BibIndexAuthorTokenizer
    |                                       BibIndexExactAuthorTokenizer
    |                                       (...)
    |
    |----BibIndexRecJsonTokenizer<---|
    |                                |
    |                        BibIndexFiletypeTokenizer
    |                        (...)
    |
    |----BibIndexMultiFieldTokenizer<---|
                                        |
                            BibIndexJournalTokenizer
                            BibIndexAuthorCountTokenizer
                            (...)
"""

from invenio.bibindex_engine_utils import (
    get_index_tags,
    get_field_indexes_memoised
)
from invenio.intbitset import intbitset
from invenio.dbquery import run_sql
import fnmatch
from invenio.bibauthority_config import (
    CFG_BIBAUTHORITY_CONTROLLED_FIELDS_BIBLIOGRAPHIC
)


class BibIndexTokenizer(object):

    """
        Base class for the tokenizers.

        Tokenizers are components that find terms which need to be
        indexed and stored in DB.
        Different types of tokenizers work in different ways.
        Tokenizers are divided into three groups:
        - tokenizers that take string as an input and split it into
          tokens/terms which later are indexed
        - tokenizers that take recID of the record and find terms
          by processing many fields/tags from the record
        - tokenizers that use bibfield module and their functions
          which precomputes terms to index
    """
    # words part

    def scan_string_for_words(self, s):
        """Return an intermediate representation of the tokens in s.

        Every tokenizer should have a scan_string function, which scans the
        input string and lexically tags its components.  These units are
        grouped together sequentially.  The output of scan_string is usually
        something like:
        {
            'TOKEN_TAG_LIST' : a list of valid keys in this output set,
            'key1' : [val1, val2, val3] - where key describes the in some
                      meaningful way
        }

        @param s: the input to be lexically tagged
        @type s: string

        @return: dict of lexically tagged input items
            In a sample Tokenizer where scan_string simply splits s on
            space, scan_string might output the following for
            "Assam and Darjeeling":
            {
                'TOKEN_TAG_LIST' : 'word_list',
                'word_list'     : ['Assam', 'and', 'Darjeeling']
            }
        @rtype: dict
        """
        raise NotImplementedError

    def parse_scanned_for_words(self, o):
        """Calculate the token list from the intermediate representation o.

        While this should be an interesting computation over the intermediate
        representation generated by scan_string, obviously in the split-on-
        space example we need only return o['word_list'].

        @param t: a dictionary with a 'word_list' key
        @type t: dict

        @return: the token items from 'word_list'
        @rtype: list of string
        """
        raise NotImplementedError

    def tokenize_for_words(self, s):
        """Main entry point.  Return token list from input string s.

        Simply composes the functionality above.

        @param s: the input to be lexically tagged
        @type s: string

        @return: the token items derived from s
        @rtype: list of string
        """
        raise NotImplementedError

    # pairs part
    def scan_string_for_pairs(self, s):
        """ See: scan_string_for_words """
        raise NotImplementedError

    def parse_scanned_for_pairs(self, o):
        """ See: parse_scanned_for_words """
        raise NotImplementedError

    def tokenize_for_pairs(self, s):
        """ See: tokenize_for_words """
        raise NotImplementedError

    # phrases part
    def scan_string_for_phrases(self, s):
        """ See: scan_string_for_words """
        raise NotImplementedError

    def parse_scanned_for_phrases(self, o):
        """ See: parse_scanned_for_words """
        raise NotImplementedError

    def tokenize_for_phrases(self, s):
        """ See: tokenize_for_words """
        raise NotImplementedError

    def get_tokenizing_function(self, wordtable_type):
        """Chooses tokenize_for_words, tokenize_for_phrases or tokenize_for_pairs
           depending on type of tokenization we want to perform."""
        raise NotImplementedError

    def get_nonmarc_tokenizing_function(self, table_type):
        """Chooses best tokenizing function
           depending on type of tokenization we want to perform.
           Non-marc version.
        """
        raise NotImplementedError

    @property
    def implemented(self):
        try:
            self.get_tokenizing_function("")
        except NotImplementedError:
            return False
        except AttributeError:
            return False
        return True

    @property
    def implemented_nonmarc(self):
        try:
            self.get_nonmarc_tokenizing_function("")
        except NotImplementedError:
            return False
        except AttributeError:
            return False
        return True

    @classmethod
    def get_modified_recids(cls, date_range, index_name):
        """ Returns all the records that need to be reindexed using this
        tokenizer **due to an action happened in the specified date range**.
        Assumes that the tokenizer is used for the index index_name.

        If a record needs to be updated due to a modification happened to
        another record, use get_dependent_recids() insthead of this method.

        @param date_range: the dates between whom this function will look for
            modified records. If the end_date is None this function will look
            for modified records after start_date
        @type date_range: tuple (start_date, end_date)
        @param index_name: the name of the index
        @type index_name: string
        @return: the modified records
        @type return: intbitset
        """
        return get_modified_recids_bibliographic(date_range, index_name)

    @classmethod
    def get_dependent_recids(cls, modified_recids, index_name):
        """ Returns all the records that need to be reindexed using this
        tokenizer **due to a modification happened to the records in
        modified_recids**.
        Assumes that the tokenizer is used for the index index_name.

        If a record needs to be updated due to an action that did not affect
        another record, use get_modified_recids() insthead of this method.

        @param modified_recids: the ids of the modified records
        @type modified_recids: intbitset
        @param index_name: the name of the index
        @type index_name: string
        @return: the dependent records
        @type return: intbitset
        """
        recids = modified_recids
        recids |= get_dependent_recids_from_authority(
            modified_recids,
            index_name
        )
        return recids


def get_modified_recids_bibliographic(dates, index_name):
    """ Finds bibliographic records that were modified between dates.
    Makes use of hstRECORD table where different revisions of record are kept.

    @param dates: the dates between whom this function will look for
        modified records. If the end_date is None this function will look
        for modified records after start_date
    @type dates: tuple (start_date, end_date)
    @param indexe_name: name of the index for reindexation
    @return: the modified recids
    @type return: intbitset
    """
    modified_recids = intbitset()

    # Get from the history all the updates happened in the specified date range
    if dates[1] is None:
        recids_info = run_sql(
            """SELECT id_bibrec, job_date, affected_fields FROM hstRECORD
               WHERE job_date >= %s""",
            (dates[0],)
        )
    else:
        recids_info = run_sql(
            """SELECT id_bibrec, job_date, affected_fields FROM hstRECORD
               WHERE job_date BETWEEN %s AND %s""",
            (dates[0], dates[1])
        )

    # Filter only records whose modified tags are associated to index_name
    for (recid, _, raw_affected_fields) in recids_info:
        affected_fields = raw_affected_fields.split(",")
        for field in affected_fields:
            if field:
                field_indexes = get_field_indexes_memoised(field) or []
                field_indexe_names = [idx[1] for idx in field_indexes]
                if index_name in field_indexe_names:
                    modified_recids.add(recid)
            else:
                # record was inserted, all fields were changed,
                # no specific affected fields
                modified_recids.add(recid)

    return modified_recids


def get_dependent_recids_from_authority(modified_recids, index_name):
    """ Searches for bibliographic records connected to authority records
    that have been changed.

    @param modified_recids: the ids of the modified records
    @type modified_recids: intbitset
    @return: the dependent records
    @type return: intbitset
    """
    from invenio.search_engine import search_unit
    from invenio.bibauthority_engine import get_control_nos_from_recID
    index_tags = get_index_tags(index_name)
    dependent_recids = intbitset()
    for tag in index_tags:
        pattern = tag.replace('%', '*')
        matches = fnmatch.filter(
            CFG_BIBAUTHORITY_CONTROLLED_FIELDS_BIBLIOGRAPHIC.keys(),
            pattern
        )

        for tag_match in matches:
            # get the type of authority record associated with this field
            auth_type = CFG_BIBAUTHORITY_CONTROLLED_FIELDS_BIBLIOGRAPHIC.get(
                tag_match
            )
            # find updated authority records of this type
            modified_auth_recIDs = search_unit(p=str(auth_type), f='980__a') \
                & modified_recids
            # now find dependent bibliographic records
            for auth_recID in modified_auth_recIDs:
                # get the fix authority identifier of this authority record
                control_nos = get_control_nos_from_recID(auth_recID)
                # there may be multiple control number entries!
                # (the '035' field is repeatable!)
                for control_no in control_nos:
                    # get the bibrec IDs that refer to AUTHORITY_ID in TAG
                    # possibly do the same for '4' subfields ?
                    tag_0 = tag_match[:5] + '0'
                    dependent_recids |= search_unit(
                        p=str(tag_0),
                        f=str(control_no)
                    )
    return dependent_recids
